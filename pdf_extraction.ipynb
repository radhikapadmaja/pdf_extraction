{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1550602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\radhi\\anaconda3\\lib\\site-packages (1.22.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29c85c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\radhi\\anaconda3\\lib\\site-packages (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow # For image extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9ecef5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabula-py\n",
      "  Downloading tabula_py-2.7.0-py3-none-any.whl (12.0 MB)\n",
      "     ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.2/12.0 MB 5.9 MB/s eta 0:00:02\n",
      "     - -------------------------------------- 0.4/12.0 MB 5.1 MB/s eta 0:00:03\n",
      "     -- ------------------------------------- 0.7/12.0 MB 5.2 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.9/12.0 MB 5.3 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.1/12.0 MB 5.2 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.2/12.0 MB 4.7 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.5/12.0 MB 4.7 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.7/12.0 MB 4.8 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.0/12.0 MB 4.9 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.3/12.0 MB 5.0 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.6/12.0 MB 5.3 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.9/12.0 MB 5.4 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 3.0/12.0 MB 5.1 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.3/12.0 MB 5.2 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.6/12.0 MB 5.2 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.0/12.0 MB 5.4 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.0 MB 5.4 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.3/12.0 MB 5.3 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.6/12.0 MB 5.3 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 4.8/12.0 MB 5.2 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.2/12.0 MB 5.3 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.4/12.0 MB 5.4 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 5.7/12.0 MB 5.5 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.0/12.0 MB 5.4 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.1/12.0 MB 5.4 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.5/12.0 MB 5.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 6.8/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.0/12.0 MB 5.4 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.4/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 7.6/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 7.8/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.2/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 8.6/12.0 MB 5.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 8.8/12.0 MB 5.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.1/12.0 MB 5.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.3/12.0 MB 5.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 9.6/12.0 MB 5.6 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 9.8/12.0 MB 5.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.1/12.0 MB 5.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 10.6/12.0 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 10.9/12.0 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.2/12.0 MB 5.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 11.7/12.0 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.0/12.0 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.0/12.0 MB 6.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from tabula-py) (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from tabula-py) (1.24.3)\n",
      "Collecting distro (from tabula-py)\n",
      "  Downloading distro-1.8.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\radhi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.25.3->tabula-py) (1.16.0)\n",
      "Installing collected packages: distro, tabula-py\n",
      "Successfully installed distro-1.8.0 tabula-py-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tabula-py # For table extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20f140ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  352k  100  352k    0     0   512k      0 --:--:-- --:--:-- --:--:--  514k\n"
     ]
    }
   ],
   "source": [
    "!curl https://arxiv.org/pdf/2308.08469.pdf --output input.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1776d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz # From PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f58c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image # From pillow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "560cc31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9cbb3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14c255f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document = fitz.open('input.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "286c7f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of pages in pdf =  9\n"
     ]
    }
   ],
   "source": [
    "#pdf_document.page_count\n",
    "print (\"No.of pages in pdf = \", len(pdf_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e092490",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pdf_document.load_page(0).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6e70d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of page 1:\n",
      "(a) Supervised Fine-Tuning (SFT):\n",
      "Autoregressive\n",
      "(b) Downstream Fine-Tuning (DFT):\n",
      "Forecasting\n",
      "Figure 2: LLM4TS framework. (a) Supervised fine-tuning uses the autoregressive approach to align the backbone model with\n",
      "time-series data. (b) Downstream fine-tuning for time-series forecasting starts with linear probing, followed by full fine-tuning.\n",
      "4.1\n",
      "Two-Stage Fine-Tuning\n",
      "Supervised Fine-tuning: Autoregressive\n",
      "Inspired by the\n",
      "achievements of chatbots, we have adopted supervised fine-\n",
      "tuning prior to downstream fine-tuning. It is well established\n",
      "that the training method for LLMs is algorithmically consis-\n",
      "tent in both pre-training on large corpus and subsequent fine-\n",
      "tuning to align with instruction-based data (Ouyang et al.\n",
      "2022). Given our selection of GPT-2 (Radford et al. 2019)\n",
      "as the backbone model, which is a causal language model,\n",
      "we ensure that the supervised fine-tuning adopts the same\n",
      "autoregressive training methodology used during its pre-\n",
      "training phase. As illustrated in Figure 2(a), given an input\n",
      "sequence of patches such as 1st patch, 2nd patch, 3rd patch,\n",
      "etc., the backbone model is expected to generate outputs cor-\n",
      "responding to the 2nd patch, 3rd patch, 4th patch, and so\n",
      "forth.\n",
      "Downstream Fine-tuning: Forecasting\n",
      "After aligning\n",
      "the LLM with patched time-series data in the supervised\n",
      "fine-tuning stage, we transfer the trained weights of the\n",
      "backbone model, including those from the encoding layers,\n",
      "to the downstream fine-tuning stage for time-series forecast-\n",
      "ing. When adapting the backbone model for the downstream\n",
      "task, two primary strategies are available: full fine-tuning\n",
      "(where all model parameters are updated) and linear prob-\n",
      "ing (where only the final linear layer is modified). Studies\n",
      "have shown that a sequential approach—initial linear prob-\n",
      "ing followed by full fine-tuning (LP-FT)—consistently sur-\n",
      "passes strategies that exclusively employ either method (Ku-\n",
      "mar et al. 2022). In our experiments, the initial half of the\n",
      "epochs is dedicated to linear probing, while the latter half\n",
      "focuses on full fine-tuning.\n",
      "4.2\n",
      "Architecture Details of LLM4TS Framework\n",
      "While the inputs for both the supervised fine-tuning stage\n",
      "and the downstream fine-tuning stage are identical, not all\n",
      "components are shared between the two. Later, we will pro-\n",
      "vide a comprehensive breakdown of the framework, tracing\n",
      "the path from the initial input to the final output across both\n",
      "stages.\n",
      "Instance Normalization\n",
      "While z-score normalization is\n",
      "standard for time-series forecasting, Reversible Instance\n",
      "Normalization (RevIN) further boosts accuracy by applying\n",
      "additional batch-specific instance normalization and subse-\n",
      "quent denormalization (Kim et al. 2021). Enhanced with a\n",
      "trainable affine transformation, RevIN has shown significant\n",
      "benefits in our downstream forecasting task.\n",
      "Since RevIN is designed for the conventional (unpatched)\n",
      "time-series format and not for the patched variant, its direct\n",
      "application to the supervised fine-tuning stage is problematic\n",
      "for two reasons:\n",
      "1. Denormalization is infeasible as outputs remain in the\n",
      "patched format rather than the unpatched format.\n",
      "2. RevIN’s trainable affine transformation is not appropriate\n",
      "for autoregressive models. Consider a scenario in which\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Content of page 1:\\n\" + text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce233b7",
   "metadata": {},
   "source": [
    "## Extract text, images and tables from the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b2c7036f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Empty DataFrame\n",
      "Columns: [to adapt successfully to tasks across 12 distinct modalities, In this section, we present our LLM4TS framework, leverag-]\n",
      "Index: [],    Supervised Fine-tuning: Autoregressive Inspired by the  Unnamed: 0  \\\n",
      "0   achievements of chatbots, we have adopted supe...             NaN   \n",
      "1   tuning prior to downstream fine-tuning. It is ...             NaN   \n",
      "2   that the training method for LLMs is algorithm...             NaN   \n",
      "3   tent in both pre-training on large corpus and ...             NaN   \n",
      "4   tuning to align with instruction-based data (O...             NaN   \n",
      "5   2022). Given our selection of GPT-2 (Radford e...             NaN   \n",
      "6   as the backbone model, which is a causal langu...             NaN   \n",
      "7   we ensure that the supervised fine-tuning adop...             NaN   \n",
      "8   autoregressive training methodology used durin...             NaN   \n",
      "9   training phase. As illustrated in Figure 2(a),...             NaN   \n",
      "10  sequence of patches such as 1st patch, 2nd pat...             NaN   \n",
      "11  etc., the backbone model is expected to genera...             NaN   \n",
      "12  responding to the 2nd patch, 3rd patch, 4th pa...             NaN   \n",
      "13                                             forth.             NaN   \n",
      "14                                                NaN             NaN   \n",
      "15  Downstream Fine-tuning: Forecasting After alig...             NaN   \n",
      "16  the LLM with patched time-series data in the s...             NaN   \n",
      "17  fine-tuning stage, we transfer the trained wei...             NaN   \n",
      "18  backbone model, including those from the encod...             NaN   \n",
      "19  to the downstream fine-tuning stage for time-s...             NaN   \n",
      "20  ing. When adapting the backbone model for the ...             NaN   \n",
      "21  task, two primary strategies are available: fu...             NaN   \n",
      "22  (where all model parameters are updated) and l...             NaN   \n",
      "23  ing (where only the final linear layer is modi...             NaN   \n",
      "24  have shown that a sequential approach—initial ...             NaN   \n",
      "25  ing followed by full fine-tuning (LP-FT)—consi...             NaN   \n",
      "\n",
      "   epochs is dedicated to linear probing, while the latter half  \n",
      "0                        focuses on full fine-tuning.            \n",
      "1                                                 NaN            \n",
      "2        4.2 Architecture Details of LLM4TS Framework            \n",
      "3                                                 NaN            \n",
      "4   While the inputs for both the supervised fine-...            \n",
      "5   and the downstream fine-tuning stage are ident...            \n",
      "6   components are shared between the two. Later, ...            \n",
      "7   vide a comprehensive breakdown of the framewor...            \n",
      "8   the path from the initial input to the final o...            \n",
      "9                                             stages.            \n",
      "10  Instance Normalization While z-score normaliza...            \n",
      "11  standard for time-series forecasting, Reversib...            \n",
      "12  Normalization (RevIN) further boosts accuracy ...            \n",
      "13  additional batch-specific instance normalizati...            \n",
      "14  quent denormalization (Kim et al. 2021). Enhan...            \n",
      "15  trainable affine transformation, RevIN has sho...            \n",
      "16       benefits in our downstream forecasting task.            \n",
      "17  Since RevIN is designed for the conventional (...            \n",
      "18  time-series format and not for the patched var...            \n",
      "19  application to the supervised fine-tuning stag...            \n",
      "20                                   for two reasons:            \n",
      "21                                                NaN            \n",
      "22  1. Denormalization is infeasible as outputs re...            \n",
      "23   patched format rather than the unpatched format.            \n",
      "24  2. RevIN’s trainable affine transformation is ...            \n",
      "25  for autoregressive models. Consider a scenario...            ,   the 4th and applying instance normalization with a train-       Datasets  \\\n",
      "0  able affine transformation, the adjusted 2nd, ...               Weather   \n",
      "1  patches are no longer suitable as ground truth...               Traffic   \n",
      "2         terations by the trainable transformation.           Electricity   \n",
      "3                                                NaN         ETTh1 & ETTh2   \n",
      "4  Given these issues, we employ standard instanc...         ETTm1 & ETTm2   \n",
      "\n",
      "   Features Timesteps Granularity  \n",
      "0        21    52,696      10 min  \n",
      "1       862    17,544      1 hour  \n",
      "2       321    26,304      1 hour  \n",
      "3         7    17,420      1 hour  \n",
      "4         7    69,680       5 min  ,    patibility with our GPT-2 backbone model. In conventional  \\\n",
      "0   NLP practices, token encoding is typically ach...          \n",
      "1   trainable lookup table to map tokens into a hi...          \n",
      "2   space. However, since our tokens are patched t...          \n",
      "3                                                 NaN          \n",
      "4   data and represent vectors rather than scalars...          \n",
      "5                                                 NaN          \n",
      "6                one-dimensional convolutional layer.          \n",
      "7                                                 NaN          \n",
      "8   For positional encoding, we use the standard a...          \n",
      "9                                                 NaN          \n",
      "10  and employ a trainable lookup table to map pat...          \n",
      "11                                                NaN          \n",
      "12  When considering temporal encoding, numerous s...          \n",
      "13                                                NaN          \n",
      "14  suggest the advantage of incorporating tempora...          \n",
      "15                                                NaN          \n",
      "16  tion with Transformer-based models in time-ser...          \n",
      "17                                                NaN          \n",
      "18  sis (Wen et al. 2022). However, when dealing w...          \n",
      "19                                                NaN          \n",
      "20  time-series data, we face two challenges due t...          \n",
      "21                                                NaN          \n",
      "22  aggregate multiple pieces of information into ...          \n",
      "23                                    representation:          \n",
      "24                                                NaN          \n",
      "25     1. Each patch encompasses multiple timestamps.          \n",
      "26  2. Each timestamp carries various temporal att...          \n",
      "27    minute, hour, day of the week, date, and month.          \n",
      "28                                                NaN          \n",
      "29  In response to the first challenge of multiple...          \n",
      "30                                                NaN          \n",
      "31  within a patch, we designate the initial times...          \n",
      "32                                                NaN          \n",
      "33  resentative. To address the second challenge a...          \n",
      "34  diverse temporal attributes within a timestamp...          \n",
      "35  a trainable lookup table for each attribute, m...          \n",
      "36  a high-dimensional space, then summing them to...          \n",
      "\n",
      "   adeptly refine the model without compromising its inherent  \n",
      "0   expressiveness. With PEFT, only 1.5% of the mo...          \n",
      "1                           parameters are trainable.          \n",
      "2                                                 NaN          \n",
      "3   Output Layer During the supervised fine-tuning...          \n",
      "4                                                 NaN          \n",
      "5   the output remains in the form of patched time...          \n",
      "6                                                 NaN          \n",
      "7   essentially a sequence of tokens. To handle th...          \n",
      "8                                                 NaN          \n",
      "9   a linear layer to modify the final dimension. ...          \n",
      "10                                                NaN          \n",
      "11  stream fine-tuning stage, the output layer tra...          \n",
      "12                                                NaN          \n",
      "13  patched time-series input into a general (unpa...          \n",
      "14                                                NaN          \n",
      "15  series format, requiring flattening before the...          \n",
      "16                                                NaN          \n",
      "17  rearranging afterward. For the output layers i...          \n",
      "18                                                NaN          \n",
      "19  we incorporate dropout immediately after the l...          \n",
      "20                                                NaN          \n",
      "21                                         formation.          \n",
      "22                                                NaN          \n",
      "23                                                NaN          \n",
      "24                                      5 Experiments          \n",
      "25  Datasets In our LLM4TS framework evaluation, w...          \n",
      "26  seven datasets: Weather, Traffic, Electricity,...          \n",
      "27  sets (ETTh1, ETTh2, ETTm1, ETTm2). These widel...          \n",
      "28  multivariate time-series datasets (Wu et al. 2...          \n",
      "29                                                NaN          \n",
      "30  thoroughly test our framework. Detailed statis...          \n",
      "31                                                NaN          \n",
      "32                  datasets are provided in Table 1.          \n",
      "33                                                NaN          \n",
      "34  Baselines For long-term time-series forecastin...          \n",
      "35  lect state-of-the-art models including GPT4TS ...          \n",
      "36  2023), DLinear (Zeng et al. 2023), PatchTST (N...          ,         Methods       LLM4TS       GPT4TS      DLinear     PatchTST  \\\n",
      "0        Metric      MSE MAE      MSE MAE      MSE MAE      MSE MAE   \n",
      "1            96  0.147 0.196  0.162 0.212  0.176 0.237  0.149 0.198   \n",
      "2           192  0.191 0.238  0.204 0.248   0.22 0.282  0.194 0.241   \n",
      "3   Weather 336  0.241 0.277  0.254 0.286  0.265 0.319  0.245 0.282   \n",
      "4           720  0.313 0.329  0.326 0.337  0.333 0.362  0.314 0.334   \n",
      "5          Avg.  0.223 0.260  0.237 0.271  0.249 0.300  0.226 0.264   \n",
      "6            96  0.371 0.394  0.376 0.397  0.375 0.399   0.37 0.399   \n",
      "7           192  0.403 0.412  0.416 0.418  0.405 0.416  0.413 0.421   \n",
      "8     ETTh1 336   0.42 0.422  0.442 0.433  0.439 0.443  0.422 0.436   \n",
      "9           720  0.422 0.444  0.477 0.456   0.472 0.49  0.447 0.466   \n",
      "10         Avg.  0.404 0.418  0.428 0.426  0.423 0.437  0.413 0.431   \n",
      "11           96  0.269 0.332  0.285 0.342  0.289 0.353  0.274 0.336   \n",
      "12          192  0.328 0.377  0.354 0.389  0.383 0.418  0.339 0.379   \n",
      "13    ETTh2 336  0.353 0.396  0.373 0.407  0.448 0.465   0.329 0.38   \n",
      "14          720  0.383 0.425  0.406 0.441  0.605 0.551  0.379 0.422   \n",
      "15         Avg.  0.333 0.383  0.355 0.395  0.431 0.447  0.330 0.379   \n",
      "16           96  0.285 0.343  0.292 0.346  0.299 0.343   0.29 0.342   \n",
      "17          192  0.324 0.366  0.332 0.372  0.335 0.365  0.332 0.369   \n",
      "18    ETTm1 336  0.353 0.385  0.366 0.394  0.369 0.386  0.366 0.392   \n",
      "19          720  0.408 0.419  0.417 0.421  0.425 0.421   0.416 0.42   \n",
      "20         Avg.  0.343 0.378  0.352 0.383  0.357 0.379  0.351 0.381   \n",
      "21           96  0.165 0.254  0.173 0.262  0.167 0.269  0.165 0.255   \n",
      "22          192   0.22 0.292  0.229 0.301  0.224 0.303   0.22 0.292   \n",
      "23    ETTm2 336  0.268 0.326  0.286 0.341  0.281 0.342  0.274 0.329   \n",
      "24          720    0.35 0.38  0.378 0.401  0.397 0.421  0.362 0.385   \n",
      "25         Avg.  0.251 0.313  0.267 0.326  0.267 0.334  0.255 0.315   \n",
      "26           96  0.128 0.223  0.139 0.238   0.14 0.237  0.129 0.222   \n",
      "27          192   0.146 0.24  0.153 0.251  0.153 0.249   0.157 0.24   \n",
      "28      ECL 336  0.163 0.258  0.169 0.266  0.169 0.267  0.163 0.259   \n",
      "29          720    0.2 0.292  0.206 0.297  0.203 0.301   0.197 0.29   \n",
      "30         Avg.  0.159 0.253  0.167 0.263  0.166 0.264  0.162 0.253   \n",
      "31           96  0.372 0.259  0.388 0.282   0.41 0.282   0.36 0.249   \n",
      "32          192  0.391 0.265   0.407 0.29  0.423 0.287  0.379 0.256   \n",
      "33  Traffic 336  0.405 0.275  0.412 0.294  0.436 0.296  0.392 0.264   \n",
      "34          720  0.437 0.292   0.45 0.312  0.466 0.315  0.432 0.286   \n",
      "35         Avg.  0.401 0.273  0.414 0.295  0.434 0.295  0.391 0.264   \n",
      "\n",
      "      FEDformer   Autoformer     Informer  \n",
      "0       MSE MAE      MSE MAE      MSE MAE  \n",
      "1   0.217 0.296  0.266 0.336    0.3 0.384  \n",
      "2   0.276 0.336  0.307 0.367  0.598 0.434  \n",
      "3    0.339 0.38  0.359 0.395  0.578 0.523  \n",
      "4   0.403 0.428  0.419 0.428  1.059 0.741  \n",
      "5   0.309 0.360  0.338 0.382  0.634 0.521  \n",
      "6   0.376 0.419  0.449 0.459  0.865 0.713  \n",
      "7    0.42 0.448    0.5 0.482  1.008 0.792  \n",
      "8   0.459 0.465  0.521 0.496  1.107 0.809  \n",
      "9   0.506 0.507  0.514 0.512  1.181 0.865  \n",
      "10  0.440 0.460  0.496 0.487  1.040 0.795  \n",
      "11  0.358 0.397  0.346 0.388  3.755 1.525  \n",
      "12  0.429 0.439  0.456 0.452  5.602 1.931  \n",
      "13  0.496 0.487  0.482 0.486  4.721 1.835  \n",
      "14  0.463 0.474  0.515 0.511  3.647 1.625  \n",
      "15  0.437 0.449  0.450 0.459  4.431 1.729  \n",
      "16  0.379 0.419  0.505 0.475  0.672 0.571  \n",
      "17  0.426 0.441  0.553 0.496  0.795 0.669  \n",
      "18  0.445 0.459  0.621 0.537  1.212 0.871  \n",
      "19   0.543 0.49  0.671 0.561  1.166 0.823  \n",
      "20  0.448 0.452  0.588 0.517  0.961 0.734  \n",
      "21  0.203 0.287  0.255 0.339  0.365 0.453  \n",
      "22  0.269 0.328   0.281 0.34  0.533 0.563  \n",
      "23  0.325 0.366  0.339 0.372  1.363 0.887  \n",
      "24  0.421 0.415  0.433 0.432  3.379 1.338  \n",
      "25  0.305 0.349  0.327 0.371  1.410 0.810  \n",
      "26  0.193 0.308  0.201 0.317  0.274 0.368  \n",
      "27  0.201 0.315  0.222 0.334  0.296 0.386  \n",
      "28  0.214 0.329  0.231 0.338    0.3 0.394  \n",
      "29  0.246 0.355  0.254 0.361  0.373 0.439  \n",
      "30  0.214 0.327  0.227 0.338  0.311 0.397  \n",
      "31  0.587 0.366  0.613 0.388  0.719 0.391  \n",
      "32  0.604 0.373  0.616 0.382  0.696 0.379  \n",
      "33  0.621 0.383  0.622 0.337   0.777 0.42  \n",
      "34  0.626 0.382   0.66 0.408  0.864 0.472  \n",
      "35  0.610 0.376  0.628 0.379  0.764 0.416  ,    Methods       LLM4TS       GPT4TS      DLinear     PatchTST    FEDformer  \\\n",
      "0   Metric      MSE MAE      MSE MAE      MSE MAE      MSE MAE      MSE MAE   \n",
      "1  Weather  0.235 0.270  0.238 0.275  0.241 0.283  0.242 0.279  0.284 0.324   \n",
      "2    ETTh1  0.525 0.493  0.590 0.525  0.691 0.600  0.633 0.542  0.639 0.561   \n",
      "3    ETTh2  0.366 0.407  0.397 0.421  0.605 0.538  0.415 0.431  0.466 0.475   \n",
      "4    ETTm1  0.408 0.413  0.464 0.441  0.411 0.429  0.501 0.466  0.722 0.605   \n",
      "5    ETTm2  0.276 0.324  0.293 0.335  0.316 0.368  0.296 0.343  0.463 0.488   \n",
      "6      ECL  0.172 0.264  0.176 0.269  0.180 0.280  0.180 0.273  0.346 0.427   \n",
      "7  Traffic  0.432 0.303  0.440 0.310  0.447 0.313  0.430 0.305  0.663 0.425   \n",
      "\n",
      "    Autoformer     Informer  \n",
      "0      MSE MAE      MSE MAE  \n",
      "1  0.300 0.342  0.597 0.495  \n",
      "2  0.702 0.596  1.199 0.809  \n",
      "3  0.488 0.499  3.872 1.513  \n",
      "4  0.802 0.628  1.192 0.821  \n",
      "5  1.342 0.930  3.370 1.440  \n",
      "6  0.431 0.478  1.195 0.891  \n",
      "7  0.749 0.446  1.534 0.811  ,    Methods       LLM4TS       GPT4TS      DLinear     PatchTST    FEDformer  \\\n",
      "0   Metric      MSE MAE      MSE MAE      MSE MAE      MSE MAE      MSE MAE   \n",
      "1  Weather  0.256 0.292  0.264 0.302  0.264 0.309  0.270 0.304  0.310 0.353   \n",
      "2    ETTh1  0.651 0.551  0.682 0.560  0.750 0.611  0.695 0.569  0.659 0.562   \n",
      "3    ETTh2  0.359 0.405  0.401 0.434  0.828 0.616  0.439 0.448  0.441 0.457   \n",
      "4    ETTm1  0.413 0.417  0.472 0.450  0.401 0.417  0.527 0.476  0.731 0.593   \n",
      "5    ETTm2  0.286 0.332  0.308 0.346  0.399 0.426  0.315 0.353  0.381 0.405   \n",
      "6      ECL  0.173 0.266  0.179 0.273  0.177 0.276  0.181 0.277  0.267 0.353   \n",
      "7  Traffic  0.418 0.295  0.434 0.305  0.451 0.317  0.418 0.297  0.677 0.424   \n",
      "\n",
      "    Autoformer     Informer  \n",
      "0      MSE MAE      MSE MAE  \n",
      "1  0.311 0.354  0.584 0.528  \n",
      "2  0.722 0.599  1.225 0.817  \n",
      "3  0.470 0.489  3.923 1.654  \n",
      "4  0.796 0.621  1.163 0.792  \n",
      "5  0.389 0.434  3.659 1.490  \n",
      "6  0.346 0.405  1.281 0.930  \n",
      "7  0.833 0.502  1.591 0.832  ,   Methods       LLM4TS     PatchTST         BTSF       TS2Vec          TNC  \\\n",
      "0  Metric      MSE MAE      MSE MAE      MSE MAE      MSE MAE      MSE MAE   \n",
      "1      24  0.315 0.365  0.322 0.369  0.541 0.519  0.599 0.534  0.632 0.596   \n",
      "2      48  0.342 0.384  0.354 0.385  0.613 0.524  0.629 0.555  0.705 0.688   \n",
      "3     168  0.401 0.415  0.419 0.424   0.64 0.532  0.755 0.636  1.097 0.993   \n",
      "4   ETTh1          NaN          NaN          NaN          NaN          NaN   \n",
      "5     336  0.421 0.427  0.445 0.446  0.864 0.689  0.907 0.717  1.454 0.919   \n",
      "6     720  0.426 0.447  0.487 0.478  0.993 0.712   1.048 0.79  1.604 1.118   \n",
      "7    Avg.  0.381 0.408  0.405 0.420  0.730 0.595  0.788 0.646  1.098 0.863   \n",
      "\n",
      "        TS-TCC  \n",
      "0      MSE MAE  \n",
      "1   0.653 0.61  \n",
      "2   0.72 0.693  \n",
      "3  1.129 1.044  \n",
      "4          NaN  \n",
      "5  1.492 1.076  \n",
      "6  1.603 1.206  \n",
      "7  1.119 0.926  ,          Methods Unnamed: 0    IMP.       LLM4TS w/o Supervised Fine-Tuning  \\\n",
      "0         Metric        NaN     MSE      MSE MAE                    MSE MAE   \n",
      "1            NaN         96   0.89%  0.371 0.394                0.372 0.395   \n",
      "2            NaN        192   1.14%  0.403 0.412                0.404 0.411   \n",
      "3   ETTh1 (full)        336   1.16%   0.42 0.422                0.422 0.423   \n",
      "4            NaN        720   2.21%  0.422 0.444                0.424 0.448   \n",
      "5            NaN       Avg.   1.37%  0.404 0.418                0.406 0.419   \n",
      "6            NaN         96   1.80%  0.417 0.432                 0.43 0.438   \n",
      "7            NaN        192   1.01%  0.469 0.468                0.488 0.474   \n",
      "8    ETTh1 (10%)        336   4.03%  0.505 0.499                0.538 0.506   \n",
      "9            NaN        720  11.89%  0.708 0.572                0.762 0.589   \n",
      "10           NaN       Avg.   6.20%  0.525 0.493                0.555 0.502   \n",
      "\n",
      "   w/o Temporal Encoding     w/o PEFT  \n",
      "0                MSE MAE      MSE MAE  \n",
      "1            0.378 0.397  0.373 0.393  \n",
      "2            0.411 0.416  0.408 0.412  \n",
      "3             0.433 0.43   0.42 0.421  \n",
      "4            0.442 0.457   0.429 0.45  \n",
      "5            0.416 0.425  0.408 0.419  \n",
      "6            0.422 0.434  0.422 0.433  \n",
      "7            0.463 0.465  0.471 0.462  \n",
      "8            0.516 0.508  0.525 0.504  \n",
      "9            0.714 0.584   0.98 0.672  \n",
      "10           0.529 0.498  0.600 0.518  ,          Methods Unnamed: 0   IMP. LLMTS w/ LP FT  LLMTS w/ FT  LLMTS w/ LP\n",
      "0         Metric        NaN    MSE        MSE MAE      MSE MAE      MSE MAE\n",
      "1            NaN         96  0.80%    0.371 0.394  0.371 0.394  0.377 0.398\n",
      "2            NaN        192  0.98%    0.403 0.412  0.404 0.413   0.41 0.416\n",
      "3   ETTh1 (full)        336  0.70%     0.42 0.422   0.42 0.423  0.426 0.424\n",
      "4            NaN        720  0.35%    0.422 0.444   0.42 0.444  0.427 0.447\n",
      "5            NaN       Avg.  0.70%    0.404 0.418  0.404 0.419  0.410 0.421\n",
      "6            NaN         96  0.12%    0.421 0.435  0.423 0.436   0.42 0.433\n",
      "7            NaN        192  2.52%    0.454 0.457  0.477 0.474  0.455 0.454\n",
      "8    ETTh1 (10%)        336  2.36%    0.515 0.504  0.545 0.524  0.511 0.507\n",
      "9            NaN        720  3.92%    0.711 0.574  0.743 0.589  0.737 0.589\n",
      "10           NaN       Avg.  2.51%    0.525 0.493  0.547 0.506  0.531 0.496,   linear probing for the initial half of the training epochs and  \\\n",
      "0  transition to full fine-tuning for the latter ...               \n",
      "1  enhances the performance of the dft models in ...               \n",
      "2  and few-shot learning, as illustrated in Table...               \n",
      "3  that few-shot learning appears to derive great...               \n",
      "4  the LP-FT method compared to standard learning...               \n",
      "5  cause few-shot scenarios are more susceptible ...               \n",
      "6  distribution shifts. Although the benefits of ...               \n",
      "\n",
      "  hance the pre-trained LLMs’ adaptability to this new modal-  \n",
      "0  ity of time-series data without distorting inh...           \n",
      "1  we incorporate Layer Normalization Tuning and ...           \n",
      "2  enhance the model’s robustness and versatility...           \n",
      "3  strategies further optimize computational effi...           \n",
      "4  ing fine-tuning. In our evaluations, LLM4TS no...           \n",
      "5  new benchmarks in long-term forecasting and re...           \n",
      "6  learning but also excels in few-shot learning,...           ]\n"
     ]
    }
   ],
   "source": [
    "def extract_text(page):\n",
    "    article = ''\n",
    "    text = page.get_text()\n",
    "    article += text\n",
    "    return article\n",
    "\n",
    "\n",
    "\n",
    "def extract_images(page, pdf_document, page_counter):\n",
    "    images = page.get_images()\n",
    "    if images:\n",
    "        img_counter = 1\n",
    "        for image_index, image in enumerate(images, start=1):            \n",
    "            xref = image[0] # first element of the list has the image meta data along with image\n",
    "            base_img = pdf_document.extract_image(xref) \n",
    "            image_data = base_img[\"image\"]\n",
    "            image_extention = base_img[\"ext\"]\n",
    "            img = PIL.Image.open(io.BytesIO(image_data))\n",
    "            img.save(open(f\"image{page_counter}{img_counter}.{image_extention}\",\"wb\"))\n",
    "            img_counter += 1\n",
    "  \n",
    " \n",
    "def extract_tables():\n",
    "    tables = tabula.read_pdf(\"input.pdf\", pages =\"all\")\n",
    "    print(tables)\n",
    "    \n",
    "    \n",
    "\n",
    "# Get text and images from all pages.\n",
    "page_counter = 1\n",
    "for i in range(len(pdf_document)):    \n",
    "    page = pdf_document[i]\n",
    "    extract_text(page)\n",
    "    extract_images(page, pdf_document, page_counter)\n",
    "    page_counter += 1    \n",
    "extract_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0af2a6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R e f e r e n c e s \\n B o m m a s a n i ,   R . ;   H u d s o n ,   D .   A . ;   A d e l i ,   E . ;   A l t m a n ,   R . ; \\n A r o r a ,   S . ;   v o n   A r x ,   S . ;   B e r n s t e i n ,   M .   S . ;   B o h g ,   J . ;   B o s s e - \\n l u t ,   A . ;   B r u n s k i l l ,   E . ;   B r y n j o l f s s o n ,   E . ;   B u c h ,   S . ;   C a r d ,   D . ; \\n C a s t e l l o n ,   R . ;   C h a t t e r j i ,   N .   S . ;   C h e n ,   A .   S . ;   C r e e l ,   K .   A . ; \\n D a v i s ,   J . ;   D e m s z k y ,   D . ;   D o n a h u e ,   C . ;   D o u m b o u y a ,   M . ;   D u r - \\n m u s ,   E . ;   E r m o n ,   S . ;   E t c h e m e n d y ,   J . ;   E t h a y a r a j h ,   K . ;   F e i - F e i , \\n L . ;   F i n n ,   C . ;   G a l e ,   T . ;   G i l l e s p i e ,   L .   E . ;   G o e l ,   K . ;   G o o d m a n , \\n N .   D . ;   G r o s s m a n ,   S . ;   G u h a ,   N . ;   H a s h i m o t o ,   T . ;   H e n d e r s o n , \\n P . ;   H e w i t t ,   J . ;   H o ,   D .   E . ;   H o n g ,   J . ;   H s u ,   K . ;   H u a n g ,   J . ;   I c a r d , \\n T .   F . ;   J a i n ,   S . ;   J u r a f s k y ,   D . ;   K a l l u r i ,   P . ;   K a r a m c h e t i ,   S . ;   K e e l - \\n i n g ,   G . ;   K h a n i ,   F . ;   K h a t t a b ,   O . ;   K o h ,   P .   W . ;   K r a s s ,   M .   S . ;   K r - \\n i s h n a ,   R . ;   K u d i t i p u d i ,   R . ;   K u m a r ,   A . ;   L a d h a k ,   F . ;   L e e ,   M . ; \\n L e e ,   T . ;   L e s k o v e c ,   J . ;   L e v e n t ,   I . ;   L i ,   X .   L . ;   L i ,   X . ;   M a ,   T . ; \\n M a l i k ,   A . ;   M a n n i n g ,   C .   D . ;   M i r c h a n d a n i ,   S . ;   M i t c h e l l ,   E . ; \\n M u n y i k w a ,   Z . ;   N a i r ,   S . ;   N a r a y a n ,   A . ;   N a r a y a n a n ,   D . ;   N e w - \\n m a n ,   B . ;   N i e ,   A . ;   N i e b l e s ,   J .   C . ;   N i l f o r o s h a n ,   H . ;   N y a r k o , \\n J .   F . ;   O g u t ,   G . ;   O r r ,   L .   J . ;   P a p a d i m i t r i o u ,   I . ;   P a r k ,   J .   S . ;   P i e c h , \\n C . ;   P o r t e l a n c e ,   E . ;   P o t t s ,   C . ;   R a g h u n a t h a n ,   A . ;   R e i c h ,   R . ; \\n R e n ,   H . ;   R o n g ,   F . ;   R o o h a n i ,   Y .   H . ;   R u i z ,   C . ;   R y a n ,   J . ;   R ’ e , \\n C . ;   S a d i g h ,   D . ;   S a g a w a ,   S . ;   S a n t h a n a m ,   K . ;   S h i h ,   A . ;   S r i n i - \\n v a s a n ,   K .   P . ;   T a m k i n ,   A . ;   T a o r i ,   R . ;   T h o m a s ,   A .   W . ;   T r a m ` e r , \\n F . ;   W a n g ,   R .   E . ;   W a n g ,   W . ;   W u ,   B . ;   W u ,   J . ;   W u ,   Y . ;   X i e , \\n S .   M . ;   Y a s u n a g a ,   M . ;   Y o u ,   J . ;   Z a h a r i a ,   M .   A . ;   Z h a n g ,   M . ; \\n Z h a n g ,   T . ;   Z h a n g ,   X . ;   Z h a n g ,   Y . ;   Z h e n g ,   L . ;   Z h o u ,   K . ;   a n d \\n L i a n g ,   P .   2 0 2 1 .   O n   t h e   O p p o r t u n i t i e s   a n d   R i s k s   o f   F o u n d a - \\n t i o n   M o d e l s .   A r X i v ,   a b s / 2 1 0 8 . 0 7 2 5 8 . \\n B r o w n ,   T .   B . ;   M a n n ,   B . ;   R y d e r ,   N . ;   S u b b i a h ,   M . ;   K a p l a n ,   J . ; \\n D h a r i w a l ,   P . ;   N e e l a k a n t a n ,   A . ;   S h y a m ,   P . ;   S a s t r y ,   G . ;   A s k e l l , \\n A . ;   A g a r w a l ,   S . ;   H e r b e r t - V o s s ,   A . ;   K r u e g e r ,   G . ;   H e n i g h a n , \\n T . ;   C h i l d ,   R . ;   R a m e s h ,   A . ;   Z i e g l e r ,   D .   M . ;   W u ,   J . ;   W i n t e r , \\n C . ;   H e s s e ,   C . ;   C h e n ,   M . ;   S i g l e r ,   E . ;   L i t w i n ,   M . ;   G r a y ,   S . ; \\n C h e s s ,   B . ;   C l a r k ,   J . ;   B e r n e r ,   C . ;   M c C a n d l i s h ,   S . ;   R a d f o r d , \\n A . ;   S u t s k e v e r ,   I . ;   a n d   A m o d e i ,   D .   2 0 2 0 .   L a n g u a g e   M o d e l s \\n a r e   F e w - S h o t   L e a r n e r s .   I n   N e u r I P S . \\n D o s o v i t s k i y ,   A . ;   B e y e r ,   L . ;   K o l e s n i k o v ,   A . ;   W e i s s e n b o r n , \\n D . ;   Z h a i ,   X . ;   U n t e r t h i n e r ,   T . ;   D e h g h a n i ,   M . ;   M i n d e r e r ,   M . ; \\n H e i g o l d ,   G . ;   G e l l y ,   S . ;   U s z k o r e i t ,   J . ;   a n d   H o u l s b y ,   N .   2 0 2 1 . \\n A n   I m a g e   i s   W o r t h   1 6 x 1 6   W o r d s :   T r a n s f o r m e r s   f o r   I m a g e \\n R e c o g n i t i o n   a t   S c a l e .   I n   I C L R .   O p e n R e v i e w . n e t . \\n E l d e l e ,   E . ;   R a g a b ,   M . ;   C h e n ,   Z . ;   W u ,   M . ;   K w o h ,   C . ;   L i ,   X . ; \\n a n d   G u a n ,   C .   2 0 2 1 .   T i m e - S e r i e s   R e p r e s e n t a t i o n   L e a r n i n g \\n v i a   T e m p o r a l   a n d   C o n t e x t u a l   C o n t r a s t i n g .   I n   I n t e r n a t i o n a l \\n J o i n t   C o n f e r e n c e   o n   A r t i f i c i a l   I n t e l l i g e n c e . \\n G a o ,   P . ;   H a n ,   J . ;   Z h a n g ,   R . ;   L i n ,   Z . ;   G e n g ,   S . ;   Z h o u ,   A . ; \\n Z h a n g ,   W . ;   L u ,   P . ;   H e ,   C . ;   Y u e ,   X . ;   L i ,   H . ;   a n d   Q i a o ,   Y .   J . \\n 2 0 2 3 .   L L a M A - A d a p t e r   V 2 :   P a r a m e t e r - E f f i c i e n t   V i s u a l   I n - \\n s t r u c t i o n   M o d e l .   A r X i v ,   a b s / 2 3 0 4 . 1 5 0 1 0 . \\n G h o s a l ,   D . ;   M a j u m d e r ,   N . ;   M e h r i s h ,   A . ;   a n d   P o r i a ,   S . \\n 2 0 2 3 . \\n T e x t - t o - A u d i o   G e n e r a t i o n   u s i n g   I n s t r u c t i o n - T u n e d \\n L L M   a n d   L a t e n t   D i f f u s i o n   M o d e l .   A r X i v ,   a b s / 2 3 0 4 . 1 3 7 3 1 . \\n H u ,   E .   J . ;   S h e n ,   Y . ;   W a l l i s ,   P . ;   A l l e n - Z h u ,   Z . ;   L i ,   Y . ;   W a n g , \\n S . ;   W a n g ,   L . ;   a n d   C h e n ,   W .   2 0 2 1 .   L o r a :   L o w - r a n k   a d a p t a t i o n \\n o f   l a r g e   l a n g u a g e   m o d e l s .   a r X i v   p r e p r i n t   a r X i v : 2 1 0 6 . 0 9 6 8 5 . \\n K i m ,   T . ;   K i m ,   J . ;   T a e ,   Y . ;   P a r k ,   C . ;   C h o i ,   J . - H . ;   a n d   C h o o ,   J . \\n 2 0 2 1 .   R e v e r s i b l e   i n s t a n c e   n o r m a l i z a t i o n   f o r   a c c u r a t e   t i m e - \\n s e r i e s   f o r e c a s t i n g   a g a i n s t   d i s t r i b u t i o n   s h i f t .   I n   I n t e r n a t i o n a l \\n C o n f e r e n c e   o n   L e a r n i n g   R e p r e s e n t a t i o n s . \\n K u m a r ,   A . ;   R a g h u n a t h a n ,   A . ;   J o n e s ,   R . ;   M a ,   T . ;   a n d \\n L i a n g ,   P .   2 0 2 2 . \\n F i n e - t u n i n g   c a n   d i s t o r t   p r e t r a i n e d   f e a - \\n t u r e s   a n d   u n d e r p e r f o r m   o u t - o f - d i s t r i b u t i o n .   a r X i v   p r e p r i n t \\n a r X i v : 2 2 0 2 . 1 0 0 5 4 . \\n L i a l i n ,   V . ;   D e s h p a n d e ,   V . ;   a n d   R u m s h i s k y ,   A .   2 0 2 3 .   S c a l - \\n i n g   D o w n   t o   S c a l e   U p :   A   G u i d e   t o   P a r a m e t e r - E f f i c i e n t   F i n e - \\n T u n i n g .   A r X i v ,   a b s / 2 3 0 3 . 1 5 6 4 7 . \\n L u ,   K . ;   G r o v e r ,   A . ;   A b b e e l ,   P . ;   a n d   M o r d a t c h ,   I .   2 0 2 1 . \\n P r e t r a i n e d   t r a n s f o r m e r s   a s   u n i v e r s a l   c o m p u t a t i o n   e n g i n e s . \\n a r X i v   p r e p r i n t   a r X i v : 2 1 0 3 . 0 5 2 4 7 ,   1 . \\n N i e ,   Y . ;   N g u y e n ,   N .   H . ;   S i n t h o n g ,   P . ;   a n d   K a l a g n a n a m ,   J . \\n 2 0 2 3 .   A   T i m e   S e r i e s   i s   W o r t h   6 4   W o r d s :   L o n g - t e r m   F o r e - \\n c a s t i n g   w i t h   T r a n s f o r m e r s .   I n   I C L R .   O p e n R e v i e w . n e t . \\n O u y a n g ,   L . ;   W u ,   J . ;   J i a n g ,   X . ;   A l m e i d a ,   D . ;   W a i n w r i g h t ,   C . ; \\n M i s h k i n ,   P . ;   Z h a n g ,   C . ;   A g a r w a l ,   S . ;   S l a m a ,   K . ;   R a y ,   A . ; \\n e t   a l .   2 0 2 2 .   T r a i n i n g   l a n g u a g e   m o d e l s   t o   f o l l o w   i n s t r u c t i o n s \\n w i t h   h u m a n   f e e d b a c k .   A d v a n c e s   i n   N e u r a l   I n f o r m a t i o n   P r o - \\n c e s s i n g   S y s t e m s ,   3 5 :   2 7 7 3 0 – 2 7 7 4 4 . \\n R a d f o r d ,   A . ;   W u ,   J . ;   C h i l d ,   R . ;   L u a n ,   D . ;   A m o d e i ,   D . ; \\n S u t s k e v e r ,   I . ;   e t   a l .   2 0 1 9 .   L a n g u a g e   m o d e l s   a r e   u n s u p e r v i s e d \\n m u l t i t a s k   l e a r n e r s .   O p e n A I   b l o g ,   1 :   9 . \\n T o n e k a b o n i ,   S . ;   E y t a n ,   D . ;   a n d   G o l d e n b e r g ,   A .   2 0 2 1 .   U n s u - \\n p e r v i s e d   R e p r e s e n t a t i o n   L e a r n i n g   f o r   T i m e   S e r i e s   w i t h   T e m - \\n p o r a l   N e i g h b o r h o o d   C o d i n g .   I n   I C L R .   O p e n R e v i e w . n e t . \\n W e n ,   Q . ;   Z h o u ,   T . ;   Z h a n g ,   C . ;   C h e n ,   W . ;   M a ,   Z . ;   Y a n ,   J . ;   a n d \\n S u n ,   L .   2 0 2 2 .   T r a n s f o r m e r s   i n   t i m e   s e r i e s :   A   s u r v e y .   a r X i v \\n p r e p r i n t   a r X i v : 2 2 0 2 . 0 7 1 2 5 . \\n W u ,   H . ;   X u ,   J . ;   W a n g ,   J . ;   a n d   L o n g ,   M .   2 0 2 1 .   A u t o f o r m e r : \\n D e c o m p o s i t i o n   t r a n s f o r m e r s   w i t h   a u t o - c o r r e l a t i o n   f o r   l o n g - \\n t e r m   s e r i e s   f o r e c a s t i n g . \\n A d v a n c e s   i n   N e u r a l   I n f o r m a t i o n \\n P r o c e s s i n g   S y s t e m s ,   3 4 :   2 2 4 1 9 – 2 2 4 3 0 . \\n Y a n g ,   L . ;   a n d   H o n g ,   S .   2 0 2 2 .   U n s u p e r v i s e d   t i m e - s e r i e s   r e p - \\n r e s e n t a t i o n   l e a r n i n g   w i t h   i t e r a t i v e   b i l i n e a r   t e m p o r a l - s p e c t r a l \\n f u s i o n .   I n   I n t e r n a t i o n a l   C o n f e r e n c e   o n   M a c h i n e   L e a r n i n g , \\n 2 5 0 3 8 – 2 5 0 5 4 .   P M L R . \\n Y u e ,   Z . ;   W a n g ,   Y . ;   D u a n ,   J . ;   Y a n g ,   T . ;   H u a n g ,   C . ;   T o n g ,   Y . ; \\n a n d   X u ,   B .   2 0 2 2 .   T s 2 v e c :   T o w a r d s   u n i v e r s a l   r e p r e s e n t a t i o n \\n o f   t i m e   s e r i e s .   I n   P r o c e e d i n g s   o f   t h e   A A A I   C o n f e r e n c e   o n \\n A r t i f i c i a l   I n t e l l i g e n c e ,   v o l u m e   3 6 ,   8 9 8 0 – 8 9 8 7 . \\n Z e n g ,   A . ;   C h e n ,   M . ;   Z h a n g ,   L . ;   a n d   X u ,   Q .   2 0 2 3 .   A r e   t r a n s - \\n f o r m e r s   e f f e c t i v e   f o r   t i m e   s e r i e s   f o r e c a s t i n g ?   I n   P r o c e e d i n g s \\n o f   t h e   A A A I   c o n f e r e n c e   o n   a r t i f i c i a l   i n t e l l i g e n c e ,   v o l u m e   3 7 , \\n 1 1 1 2 1 – 1 1 1 2 8 . \\n Z h a n g ,   Y . ;   G o n g ,   K . ;   Z h a n g ,   K . ;   L i ,   H . ;   Q i a o ,   Y . ;   O u y a n g , \\n W . ;   a n d   Y u e ,   X .   2 0 2 3 . \\n M e t a - T r a n s f o r m e r :   A   U n i f i e d \\n F r a m e w o r k   f o r   M u l t i m o d a l   L e a r n i n g . \\n a r X i v   p r e p r i n t \\n a r X i v : 2 3 0 7 . 1 0 8 0 2 . \\n Z h o u ,   H . ;   Z h a n g ,   S . ;   P e n g ,   J . ;   Z h a n g ,   S . ;   L i ,   J . ;   X i o n g ,   H . ; \\n a n d   Z h a n g ,   W .   2 0 2 1 .   I n f o r m e r :   B e y o n d   e f f i c i e n t   t r a n s f o r m e r \\n f o r   l o n g   s e q u e n c e   t i m e - s e r i e s   f o r e c a s t i n g .   I n   P r o c e e d i n g s   o f \\n t h e   A A A I   c o n f e r e n c e   o n   a r t i f i c i a l   i n t e l l i g e n c e ,   v o l u m e   3 5 , \\n 1 1 1 0 6 – 1 1 1 1 5 . \\n Z h o u ,   T . ;   M a ,   Z . ;   W e n ,   Q . ;   W a n g ,   X . ;   S u n ,   L . ;   a n d   J i n ,   R . \\n 2 0 2 2 .   F e d f o r m e r :   F r e q u e n c y   e n h a n c e d   d e c o m p o s e d   t r a n s - \\n f o r m e r   f o r   l o n g - t e r m   s e r i e s   f o r e c a s t i n g . \\n I n   I n t e r n a t i o n a l \\n C o n f e r e n c e   o n   M a c h i n e   L e a r n i n g ,   2 7 2 6 8 – 2 7 2 8 6 .   P M L R . \\nLLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained\\nLLMs\\nChing Chang, Wen-Chih Peng, Tien-Fu Chen\\nNational Yang Ming Chiao Tung University\\nblacksnail789521.cs10@nycu.edu.tw, {wcpeng, tfchen}@cs.nycu.edu.tw\\nAbstract\\nIn this work, we leverage pre-trained Large Language Mod-\\nels (LLMs) to enhance time-series forecasting. Mirroring\\nthe growing interest in unifying models for Natural Lan-\\nguage Processing and Computer Vision, we envision creat-\\ning an analogous model for long-term time-series forecast-\\ning. Due to limited large-scale time-series data for building\\nrobust foundation models, our approach LLM4TS focuses\\non leveraging the strengths of pre-trained LLMs. By com-\\nbining time-series patching with temporal encoding, we have\\nenhanced the capability of LLMs to handle time-series data\\neffectively. Inspired by the supervised fine-tuning in chatbot\\ndomains, we prioritize a two-stage fine-tuning process: first\\nconducting supervised fine-tuning to orient the LLM towards\\ntime-series data, followed by task-specific downstream fine-\\ntuning. Furthermore, to unlock the flexibility of pre-trained\\nLLMs without extensive parameter adjustments, we adopt\\nseveral Parameter-Efficient Fine-Tuning (PEFT) techniques.\\nDrawing on these innovations, LLM4TS has yielded state-of-\\nthe-art results in long-term forecasting. Our model has also\\nshown exceptional capabilities as both a robust representa-\\ntion learner and an effective few-shot learner, thanks to the\\nknowledge transferred from the pre-trained LLM.\\n1\\nIntroduction\\nMultivariate time-series data is prevalent across various do-\\nmains. In many of these areas, forecasting stands out as one\\nof the most crucial tasks, largely because it eliminates the\\nneed for manual labeling. There are numerous methods de-\\nsigned specifically for time-series forecasting (Zeng et al.\\n2023; Nie et al. 2023; Wu et al. 2021; Zhou et al. 2022,\\n2021), though some lean towards unsupervised representa-\\ntion learning (Yang and Hong 2022; Yue et al. 2022; Tonek-\\naboni, Eytan, and Goldenberg 2021; Eldele et al. 2021). Re-\\ngardless, the ultimate goal of attaining strong time-series\\nforecasting performance lies in employing adept represen-\\ntation learners: first extracting meaningful representations\\nfrom the time-series data and then using these learned rep-\\nresentations for forecasting.\\nWe have observed the emergence of foundation models in\\nNatural Language Processing (NLP) (Radford et al. 2019;\\nBrown et al. 2020) and Computer Vision (CV) (Dosovitskiy\\net al. 2021), with numerous successful applications lever-\\naging these models for advanced representation learning.\\nGiven the limited availability of large-scale time-series data\\nto train robust foundation models, we aim to investigate the\\npotential of utilizing pre-trained Large Language Models\\n(LLMs) as powerful representation learners. Notably, LLMs\\nare recognized for their exceptional few-shot learning capa-\\nbilities, making them ideal for real-world scenarios where\\ncollecting vast amounts of training data is a challenge.\\nTo integrate LLMs with time-series data, we need to ad-\\ndress two pivotal questions: 1. How can we input time-series\\ndata into LLMs? 2. How can we utilize pre-trained LLMs\\nwithout distorting their inherent features?\\nHow can we input time-series data into LLMs?\\nTo ac-\\ncommodate new data modalities within LLMs, it is es-\\nsential to tokenize the data. Recent research (Zhang et al.\\n2023) highlights patching a promising tokenization ap-\\nproach, which is relevant for a variety of data types in-\\ncluding images, audio, and time-series data. PatchTST (Nie\\net al. 2023) demonstrates that patching time-series data\\nwith channel-independence often yields improved results.\\nChannel-independence entails treating multivariate time-\\nseries data as multiple univariate time-series data, and these\\nare then processed by a single model. Furthermore, several\\nTransformer-based time-series forecasting models suggest\\nthat integrating temporal information can enhance outcomes\\n(Wen et al. 2022). As such, we introduce a novel approach\\nthat integrates temporal information while employing the\\ntechniques of patching and channel-independence.\\nHow can we utilize pre-trained LLMs without distort-\\ning their inherent features?\\nBuilding high-quality chat-\\nbots like InstructGPT (Ouyang et al. 2022) and ChatGPT\\nrequires strategic alignment of a pre-trained model with\\ninstruction-based data through supervised fine-tuning. This\\nmethod ensures the model becomes familiarized with tar-\\nget data formats and characteristics. Motivated by this suc-\\ncess, we introduce a two-stage fine-tuning approach. We\\nstart by guiding the LLM towards time-series data using su-\\npervised fine-tuning, then pivot to downstream fine-tuning\\ngeared towards time-series forecasting. Alongside this two-\\nstage fine-tuning approach, there is a need to enhance the\\npre-trained LLMs’ adaptability to new data modalities with-\\nout distorting models’ inherent features. We incorporate two\\nParameter-Efficient Fine-Tuning (PEFT) techniques, Layer\\nNormalization Tuning (Lu et al. 2021) and LoRA (Hu et al.\\n2021), to optimize model flexibility without extensive pa-\\narXiv:2308.08469v1  [cs.LG]  16 Aug 2023\\nrameter adjustments in the backbone model.\\nIn summary, we introduce LLM4TS, a framework for\\ntime-series forecasting with pre-trained LLMs. Our main\\ncontributions can be listed as follows:\\n• Integration of Time-Series with LLMs: We utilize\\npatching and channel-independence to tokenize time-\\nseries data, and we introduce a novel approach to inte-\\ngrate temporal information with patching.\\n• Adaptable Fine-Tuning for LLMs: We present a two-\\nstage fine-tuning methodology: an initial supervised fine-\\ntuning stage to align LLMs with time-series data char-\\nacteristics, succeeded by a downstream fine-tuning stage\\ndedicated to time-series forecasting.\\n• Optimized Model Flexibility: To ensure both robust-\\nness and adaptability in pre-trained LLMs for time-series\\ndata, we incorporate two PEFT techniques, Layer Nor-\\nmalization Tuning and LoRA.\\n• Real-World Application Relevance: Our methods cater\\nto the challenges of limited time-series data availability,\\nleveraging the few-shot learning capabilities of LLMs,\\nto make our approach highly relevant for practical, real-\\nworld forecasting scenarios.\\n2\\nRelated Work\\n2.1\\nIn-Modality Knowledge Transfer\\nOne standout benefit of using foundation models is their\\ncapability to transfer knowledge to downstream tasks. The\\ntransformation of LLMs into chatbots serves as a testament\\nto this success. Prominent models like InstructGPT (Ouyang\\net al. 2022) and ChatGPT employ supervised fine-tuning to\\nalign with instruction-based data. While the benefits of fine-\\ntuning are substantial, the computational burden of refining\\nan entire model can be significant. This has led to the rise of\\nPEFT as a popular technique to reduce costs (Lialin, Desh-\\npande, and Rumshisky 2023). LLaMA-Adapter (Gao et al.\\n2023) achieves ChatGPT-level performance by fine-tuning\\na mere 0.02% of its parameters. In LLM4TS, we integrate\\nsupervised fine-tuning and PEFT to harness the potential of\\npretrained LLMs without the need for significant computa-\\ntional resources.\\n2.2\\nCross-Modality Knowledge Transfer\\nLeveraging the prowess of foundation models in NLP and\\nCV, we can extend their knowledge transfer capabilities\\nacross diverse data modalities. In NLP, numerous research\\nhas demonstrated the potential of pre-trained LLMs in tasks\\ninvolving images (Lu et al. 2021), audio (Ghosal et al.\\n2023), and time-series data (Zhou et al. 2023). In CV, pre-\\ntrained ViT (Dosovitskiy et al. 2021) has been demonstrated\\nto adapt successfully to tasks across 12 distinct modalities\\n(Zhang et al. 2023). With LLM4TS, we utilize pretrained\\nLLM expertise to address challenges in time-series data.\\n2.3\\nLong-term Time-Series Forecasting\\nNumerous efforts have been dedicated to employing Trans-\\nformer models for long-term time-series forecasting (Nie\\net al. 2023; Zhou et al. 2021; Wu et al. 2021; Zhou et al.\\nFigure 1: Problem formulation for multivariate time-series\\nforecasting.\\n2022). While Transformer-based models have gained trac-\\ntion, DLinear (Zeng et al. 2023) reveals that an embarrass-\\ningly simple single-layer linear model can surpass many\\nof these sophisticated Transformer-based approaches. As\\nwe will illustrate, LLM4TS sets new benchmarks alongside\\nthese state-of-the-art approaches.\\n2.4\\nTime-Series Representation Learning\\nIn the time-series domain, self-supervised learning emerges\\nas a prominent approach to representation learning. While\\nTransformers are widely recognized as prime candidates\\nfor foundation models (Bommasani et al. 2021), they don’t\\nconsistently stand out as the preferred architecture in time-\\nseries self-supervised learning. Instead, CNN-based (Yue\\net al. 2022) or RNN-based (Tonekaboni, Eytan, and Golden-\\nberg 2021) backbones continue to be favored for such tasks.\\nLLM4TS seeks to reestablish the Transformer’s dominance\\nin this realm of self-supervised learning.\\n3\\nProblem Formulation\\nGiven a complete and evenly-sampled multivariate time se-\\nries, we use a sliding data window to extract sequential sam-\\nples, as illustrated in Figure 1. This window moves with a\\nstride of 1 and has a total length of L + T — comprising\\npast data XP = (x1, . . . , xL) with a look-back window\\nlength L and future data XF = (xL+1, . . . , xL+T ) with a\\nprediction length T. For each time step t, xt represents an\\nM-dimensional vector. Our objective is to use the past data\\nXP to predict the future data XF .\\n4\\nMethod\\nIn this section, we present our LLM4TS framework, leverag-\\ning the pre-trained GPT-2 (Radford et al. 2019) as the back-\\nbone model. In Section 4.1, we introduce the two-stage fine-\\ntuning training strategy. Following that, Section 4.2 details\\nthe architecture, covering instance normalization, patching,\\nchannel-independence, three distinct encodings, the back-\\nbone model structure, and the output layer. All key compo-\\nnents of our framework are illustrated in Figure 2.\\n(a) Supervised Fine-Tuning (SFT):\\nAutoregressive\\n(b) Downstream Fine-Tuning (DFT):\\nForecasting\\nFigure 2: LLM4TS framework. (a) Supervised fine-tuning uses the autoregressive approach to align the backbone model with\\ntime-series data. (b) Downstream fine-tuning for time-series forecasting starts with linear probing, followed by full fine-tuning.\\n4.1\\nTwo-Stage Fine-Tuning\\nSupervised Fine-tuning: Autoregressive\\nInspired by the\\nachievements of chatbots, we have adopted supervised fine-\\ntuning prior to downstream fine-tuning. It is well established\\nthat the training method for LLMs is algorithmically consis-\\ntent in both pre-training on large corpus and subsequent fine-\\ntuning to align with instruction-based data (Ouyang et al.\\n2022). Given our selection of GPT-2 (Radford et al. 2019)\\nas the backbone model, which is a causal language model,\\nwe ensure that the supervised fine-tuning adopts the same\\nautoregressive training methodology used during its pre-\\ntraining phase. As illustrated in Figure 2(a), given an input\\nsequence of patches such as 1st patch, 2nd patch, 3rd patch,\\netc., the backbone model is expected to generate outputs cor-\\nresponding to the 2nd patch, 3rd patch, 4th patch, and so\\nforth.\\nDownstream Fine-tuning: Forecasting\\nAfter aligning\\nthe LLM with patched time-series data in the supervised\\nfine-tuning stage, we transfer the trained weights of the\\nbackbone model, including those from the encoding layers,\\nto the downstream fine-tuning stage for time-series forecast-\\ning. When adapting the backbone model for the downstream\\ntask, two primary strategies are available: full fine-tuning\\n(where all model parameters are updated) and linear prob-\\ning (where only the final linear layer is modified). Studies\\nhave shown that a sequential approach—initial linear prob-\\ning followed by full fine-tuning (LP-FT)—consistently sur-\\npasses strategies that exclusively employ either method (Ku-\\nmar et al. 2022). In our experiments, the initial half of the\\nepochs is dedicated to linear probing, while the latter half\\nfocuses on full fine-tuning.\\n4.2\\nArchitecture Details of LLM4TS Framework\\nWhile the inputs for both the supervised fine-tuning stage\\nand the downstream fine-tuning stage are identical, not all\\ncomponents are shared between the two. Later, we will pro-\\nvide a comprehensive breakdown of the framework, tracing\\nthe path from the initial input to the final output across both\\nstages.\\nInstance Normalization\\nWhile z-score normalization is\\nstandard for time-series forecasting, Reversible Instance\\nNormalization (RevIN) further boosts accuracy by applying\\nadditional batch-specific instance normalization and subse-\\nquent denormalization (Kim et al. 2021). Enhanced with a\\ntrainable affine transformation, RevIN has shown significant\\nbenefits in our downstream forecasting task.\\nSince RevIN is designed for the conventional (unpatched)\\ntime-series format and not for the patched variant, its direct\\napplication to the supervised fine-tuning stage is problematic\\nfor two reasons:\\n1. Denormalization is infeasible as outputs remain in the\\npatched format rather than the unpatched format.\\n2. RevIN’s trainable affine transformation is not appropriate\\nfor autoregressive models. Consider a scenario in which\\nwe aim to use the 1st, 2nd, and 3rd patches to predict\\nthe 2nd, 3rd, and 4th patches. After assembling the nec-\\nessary time-series data to create patches from the 1st to\\nthe 4th and applying instance normalization with a train-\\nable affine transformation, the adjusted 2nd, 3rd, and 4th\\npatches are no longer suitable as ground truth due to al-\\nterations by the trainable transformation.\\nGiven these issues, we employ standard instance normaliza-\\ntion during the supervised fine-tuning stage.\\nPatching and Channel-Independence\\nTo adapt time-\\nseries data for Transformer-based models, we tokenize it us-\\ning PatchTST’s (Nie et al. 2023) method, which employs\\nchannel-independence and patching. Channel-independence\\ntreats multivariate time-series as multiple univariate time-\\nseries data, and these are then processed by a single model.\\nWhile channel-mixing models aim to exploit cross-channel\\ndata directly, channel-independence frequently delivers su-\\nperior performance by indirectly capturing cross-channel in-\\nteractions through weight sharing. This is because channel-\\nmixing often suffers from limited data and overfitting.\\nWith channel-independence applied, the model perceives\\ndata as univariate time-series. The subsequent patching step\\ngroups adjacent time steps into a singular patch-based token.\\nThis approach broadens the input’s historical scope without\\nincreasing token length, providing time and space advan-\\ntages crucial for resource-intensive LLMs.\\nThree Encodings\\nGiven a series of tokens, it is necessary\\nto apply token encoding to transform these tokens for com-\\npatibility with our GPT-2 backbone model. In conventional\\nNLP practices, token encoding is typically achieved using a\\ntrainable lookup table to map tokens into a high-dimensional\\nspace. However, since our tokens are patched time-series\\ndata and represent vectors rather than scalars, we opt for a\\none-dimensional convolutional layer.\\nFor positional encoding, we use the standard approach\\nand employ a trainable lookup table to map patch locations.\\nWhen considering temporal encoding, numerous studies\\nsuggest the advantage of incorporating temporal informa-\\ntion with Transformer-based models in time-series analy-\\nsis (Wen et al. 2022). However, when dealing with patched\\ntime-series data, we face two challenges due to the need to\\naggregate multiple pieces of information into one unified\\nrepresentation:\\n1. Each patch encompasses multiple timestamps.\\n2. Each timestamp carries various temporal attributes like\\nminute, hour, day of the week, date, and month.\\nIn response to the first challenge of multiple timestamps\\nwithin a patch, we designate the initial timestamp as its rep-\\nresentative. To address the second challenge associated with\\ndiverse temporal attributes within a timestamp, we employ\\na trainable lookup table for each attribute, mapping it into\\na high-dimensional space, then summing them to produce a\\nsingular temporal embedding.\\nFinally, the token, positional, and temporal embeddings\\nare summed to yield the final embedding, which is then fed\\ninto the pre-trained LLM.\\nTable 1: Statistical overview of the 7 datasets for long-term\\ntime-series forecasting.\\nDatasets\\nFeatures\\nTimesteps\\nGranularity\\nWeather\\n21\\n52,696\\n10 min\\nTraffic\\n862\\n17,544\\n1 hour\\nElectricity\\n321\\n26,304\\n1 hour\\nETTh1 & ETTh2\\n7\\n17,420\\n1 hour\\nETTm1 & ETTm2\\n7\\n69,680\\n5 min\\nPre-Trained LLM and PEFT\\nIn our setup, GPT-2 (Rad-\\nford et al. 2019) serves as the backbone model for our pre-\\ntrained LLM. To retain the model’s foundational knowl-\\nedge, we have frozen the majority of the parameters, particu-\\nlarly those associated with the multi-head attention and feed-\\nforward layers within the Transformer block. Beyond the\\nmore cost-effective computational advantages, many stud-\\nies indicate that retaining most parameters as non-trainable\\noften yields better results than training a pre-trained LLM\\nfrom scratch (Lu et al. 2021; Zhou et al. 2023).\\nFor the remaining trainable parameters in the pre-trained\\nLLM, we employ PEFT techniques as efficient approaches\\nto selectively adjust or introduce a limited set of trainable\\nparameters. We utilize the selection-based method, Layer\\nNormalization Tuning (Lu et al. 2021), to adjust pre-existing\\nparameters by making the affine transformation in layer nor-\\nmalization trainable. Concurrently, we employ LoRA (Low-\\nRank Adaptation) (Hu et al. 2021), a reparameterization-\\nbased method that leverages low-rank representations to\\nadeptly refine the model without compromising its inherent\\nexpressiveness. With PEFT, only 1.5% of the model’s total\\nparameters are trainable.\\nOutput Layer\\nDuring the supervised fine-tuning stage,\\nthe output remains in the form of patched time-series data,\\nessentially a sequence of tokens. To handle this, we employ\\na linear layer to modify the final dimension. In the down-\\nstream fine-tuning stage, the output layer transforms the\\npatched time-series input into a general (unpatched) time-\\nseries format, requiring flattening before the linear layer and\\nrearranging afterward. For the output layers in both stages,\\nwe incorporate dropout immediately after the linear trans-\\nformation.\\n5\\nExperiments\\nDatasets\\nIn our LLM4TS framework evaluation, we use\\nseven datasets: Weather, Traffic, Electricity, and four ETT\\nsets (ETTh1, ETTh2, ETTm1, ETTm2). These widely-used\\nmultivariate time-series datasets (Wu et al. 2021) help us\\nthoroughly test our framework. Detailed statistics for these\\ndatasets are provided in Table 1.\\nBaselines\\nFor long-term time-series forecasting, we se-\\nlect state-of-the-art models including GPT4TS (Zhou et al.\\n2023), DLinear (Zeng et al. 2023), PatchTST (Nie et al.\\n2023), FEDformer (Zhou et al. 2022), Autoformer (Wu et al.\\n2021), and Informer (Zhou et al. 2021). The same set of\\nmodels is used for few-shot learning and ablation study. For\\nself-supervised learning, we choose PatchTST, BTSF (Yang\\nTable 2: Long-term forecasting for multivariate time-series data. We use prediction lengths T ∈ {96, 192, 336, 720} for all\\ndatasets. The best average results are in bold, while the second-best are underlined.\\nMethods\\nLLM4TS\\nGPT4TS\\nDLinear\\nPatchTST\\nFEDformer\\nAutoformer\\nInformer\\nMetric\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nWeather\\n96\\n0.147\\n0.196\\n0.162\\n0.212\\n0.176\\n0.237\\n0.149\\n0.198\\n0.217\\n0.296\\n0.266\\n0.336\\n0.3\\n0.384\\n192\\n0.191\\n0.238\\n0.204\\n0.248\\n0.22\\n0.282\\n0.194\\n0.241\\n0.276\\n0.336\\n0.307\\n0.367\\n0.598\\n0.434\\n336\\n0.241\\n0.277\\n0.254\\n0.286\\n0.265\\n0.319\\n0.245\\n0.282\\n0.339\\n0.38\\n0.359\\n0.395\\n0.578\\n0.523\\n720\\n0.313\\n0.329\\n0.326\\n0.337\\n0.333\\n0.362\\n0.314\\n0.334\\n0.403\\n0.428\\n0.419\\n0.428\\n1.059\\n0.741\\nAvg.\\n0.223\\n0.260\\n0.237\\n0.271\\n0.249\\n0.300\\n0.226\\n0.264\\n0.309\\n0.360\\n0.338\\n0.382\\n0.634\\n0.521\\nETTh1\\n96\\n0.371\\n0.394\\n0.376\\n0.397\\n0.375\\n0.399\\n0.37\\n0.399\\n0.376\\n0.419\\n0.449\\n0.459\\n0.865\\n0.713\\n192\\n0.403\\n0.412\\n0.416\\n0.418\\n0.405\\n0.416\\n0.413\\n0.421\\n0.42\\n0.448\\n0.5\\n0.482\\n1.008\\n0.792\\n336\\n0.42\\n0.422\\n0.442\\n0.433\\n0.439\\n0.443\\n0.422\\n0.436\\n0.459\\n0.465\\n0.521\\n0.496\\n1.107\\n0.809\\n720\\n0.422\\n0.444\\n0.477\\n0.456\\n0.472\\n0.49\\n0.447\\n0.466\\n0.506\\n0.507\\n0.514\\n0.512\\n1.181\\n0.865\\nAvg.\\n0.404\\n0.418\\n0.428\\n0.426\\n0.423\\n0.437\\n0.413\\n0.431\\n0.440\\n0.460\\n0.496\\n0.487\\n1.040\\n0.795\\nETTh2\\n96\\n0.269\\n0.332\\n0.285\\n0.342\\n0.289\\n0.353\\n0.274\\n0.336\\n0.358\\n0.397\\n0.346\\n0.388\\n3.755\\n1.525\\n192\\n0.328\\n0.377\\n0.354\\n0.389\\n0.383\\n0.418\\n0.339\\n0.379\\n0.429\\n0.439\\n0.456\\n0.452\\n5.602\\n1.931\\n336\\n0.353\\n0.396\\n0.373\\n0.407\\n0.448\\n0.465\\n0.329\\n0.38\\n0.496\\n0.487\\n0.482\\n0.486\\n4.721\\n1.835\\n720\\n0.383\\n0.425\\n0.406\\n0.441\\n0.605\\n0.551\\n0.379\\n0.422\\n0.463\\n0.474\\n0.515\\n0.511\\n3.647\\n1.625\\nAvg.\\n0.333\\n0.383\\n0.355\\n0.395\\n0.431\\n0.447\\n0.330\\n0.379\\n0.437\\n0.449\\n0.450\\n0.459\\n4.431\\n1.729\\nETTm1\\n96\\n0.285\\n0.343\\n0.292\\n0.346\\n0.299\\n0.343\\n0.29\\n0.342\\n0.379\\n0.419\\n0.505\\n0.475\\n0.672\\n0.571\\n192\\n0.324\\n0.366\\n0.332\\n0.372\\n0.335\\n0.365\\n0.332\\n0.369\\n0.426\\n0.441\\n0.553\\n0.496\\n0.795\\n0.669\\n336\\n0.353\\n0.385\\n0.366\\n0.394\\n0.369\\n0.386\\n0.366\\n0.392\\n0.445\\n0.459\\n0.621\\n0.537\\n1.212\\n0.871\\n720\\n0.408\\n0.419\\n0.417\\n0.421\\n0.425\\n0.421\\n0.416\\n0.42\\n0.543\\n0.49\\n0.671\\n0.561\\n1.166\\n0.823\\nAvg.\\n0.343\\n0.378\\n0.352\\n0.383\\n0.357\\n0.379\\n0.351\\n0.381\\n0.448\\n0.452\\n0.588\\n0.517\\n0.961\\n0.734\\nETTm2\\n96\\n0.165\\n0.254\\n0.173\\n0.262\\n0.167\\n0.269\\n0.165\\n0.255\\n0.203\\n0.287\\n0.255\\n0.339\\n0.365\\n0.453\\n192\\n0.22\\n0.292\\n0.229\\n0.301\\n0.224\\n0.303\\n0.22\\n0.292\\n0.269\\n0.328\\n0.281\\n0.34\\n0.533\\n0.563\\n336\\n0.268\\n0.326\\n0.286\\n0.341\\n0.281\\n0.342\\n0.274\\n0.329\\n0.325\\n0.366\\n0.339\\n0.372\\n1.363\\n0.887\\n720\\n0.35\\n0.38\\n0.378\\n0.401\\n0.397\\n0.421\\n0.362\\n0.385\\n0.421\\n0.415\\n0.433\\n0.432\\n3.379\\n1.338\\nAvg.\\n0.251\\n0.313\\n0.267\\n0.326\\n0.267\\n0.334\\n0.255\\n0.315\\n0.305\\n0.349\\n0.327\\n0.371\\n1.410\\n0.810\\nECL\\n96\\n0.128\\n0.223\\n0.139\\n0.238\\n0.14\\n0.237\\n0.129\\n0.222\\n0.193\\n0.308\\n0.201\\n0.317\\n0.274\\n0.368\\n192\\n0.146\\n0.24\\n0.153\\n0.251\\n0.153\\n0.249\\n0.157\\n0.24\\n0.201\\n0.315\\n0.222\\n0.334\\n0.296\\n0.386\\n336\\n0.163\\n0.258\\n0.169\\n0.266\\n0.169\\n0.267\\n0.163\\n0.259\\n0.214\\n0.329\\n0.231\\n0.338\\n0.3\\n0.394\\n720\\n0.2\\n0.292\\n0.206\\n0.297\\n0.203\\n0.301\\n0.197\\n0.29\\n0.246\\n0.355\\n0.254\\n0.361\\n0.373\\n0.439\\nAvg.\\n0.159\\n0.253\\n0.167\\n0.263\\n0.166\\n0.264\\n0.162\\n0.253\\n0.214\\n0.327\\n0.227\\n0.338\\n0.311\\n0.397\\nTraffic\\n96\\n0.372\\n0.259\\n0.388\\n0.282\\n0.41\\n0.282\\n0.36\\n0.249\\n0.587\\n0.366\\n0.613\\n0.388\\n0.719\\n0.391\\n192\\n0.391\\n0.265\\n0.407\\n0.29\\n0.423\\n0.287\\n0.379\\n0.256\\n0.604\\n0.373\\n0.616\\n0.382\\n0.696\\n0.379\\n336\\n0.405\\n0.275\\n0.412\\n0.294\\n0.436\\n0.296\\n0.392\\n0.264\\n0.621\\n0.383\\n0.622\\n0.337\\n0.777\\n0.42\\n720\\n0.437\\n0.292\\n0.45\\n0.312\\n0.466\\n0.315\\n0.432\\n0.286\\n0.626\\n0.382\\n0.66\\n0.408\\n0.864\\n0.472\\nAvg.\\n0.401\\n0.273\\n0.414\\n0.295\\n0.434\\n0.295\\n0.391\\n0.264\\n0.610\\n0.376\\n0.628\\n0.379\\n0.764\\n0.416\\nand Hong 2022), TS2Vec (Yue et al. 2022), TNC (Tonek-\\naboni, Eytan, and Goldenberg 2021), and TS-TCC (Eldele\\net al. 2021). Consistent with prior research (Zeng et al. 2023;\\nNie et al. 2023; Zhou et al. 2023), we rely on Mean Squared\\nError (MSE) and Mean Absolute Error (MAE) as evaluation\\nmetrics across all experiments.\\nImplementation Details\\nFor our experiments in long-\\nterm time-series forecasting, few-shot learning, and ablation\\nstudy, we use the configurations from PatchTST (Nie et al.\\n2023) for a consistent comparison. Specifically, our look-\\nback window length L is set to either 336 or 512, and we\\nreport the best results. With PatchTST’s settings for patch-\\ning, we set patch length P = 16 and stride S = 8. For\\nself-supervised learning, the settings are slightly adjusted to\\nL = 512, P = 12, and S = 12. Aligned with the GPT4TS\\nconfiguration (Zhou et al. 2023), we utilize only the first 6\\nlayers of the 12-layer GPT-2 base (Radford et al. 2019).\\n5.1\\nLong-Term Time-Series Forecasting\\nFor all datasets, we present results using a consistent pre-\\ndiction length set T ∈ {96, 192, 336, 720}. Given our two-\\nstage fine-tuning approach, models are labeled as sft mod-\\nels after supervised fine-tuning and as dft models after\\ndownstream fine-tuning. On each dataset, a single sft model\\nis utilized across all prediction lengths, while a distinct dft\\nmodel is deployed for each prediction length. For dft mod-\\nels within the same dataset, we maintain consistent hyperpa-\\nrameters across different prediction lengths. The results are\\npresented in Table 2. While the primary intent of using the\\npretrained LLM is for few-shot learning, LLM4TS still sur-\\npasses most baseline methods even when given access to the\\nfull dataset. Notably, LLM4TS claims the leading position\\nin 9 of the 14 evaluations, covering 7 datasets and 2 metrics.\\nTable 3: Few-shot long-term forecasting using 10% of the training data. Results are averaged across four prediction lengths:\\nT ∈ {96, 192, 336, 720} for all datasets. The best average results are in bold, while the second-best are underlined.\\nMethods\\nLLM4TS\\nGPT4TS\\nDLinear\\nPatchTST\\nFEDformer\\nAutoformer\\nInformer\\nMetric\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nWeather\\n0.235\\n0.270\\n0.238\\n0.275\\n0.241\\n0.283\\n0.242\\n0.279\\n0.284\\n0.324\\n0.300\\n0.342\\n0.597\\n0.495\\nETTh1\\n0.525\\n0.493\\n0.590\\n0.525\\n0.691\\n0.600\\n0.633\\n0.542\\n0.639\\n0.561\\n0.702\\n0.596\\n1.199\\n0.809\\nETTh2\\n0.366\\n0.407\\n0.397\\n0.421\\n0.605\\n0.538\\n0.415\\n0.431\\n0.466\\n0.475\\n0.488\\n0.499\\n3.872\\n1.513\\nETTm1\\n0.408\\n0.413\\n0.464\\n0.441\\n0.411\\n0.429\\n0.501\\n0.466\\n0.722\\n0.605\\n0.802\\n0.628\\n1.192\\n0.821\\nETTm2\\n0.276\\n0.324\\n0.293\\n0.335\\n0.316\\n0.368\\n0.296\\n0.343\\n0.463\\n0.488\\n1.342\\n0.930\\n3.370\\n1.440\\nECL\\n0.172\\n0.264\\n0.176\\n0.269\\n0.180\\n0.280\\n0.180\\n0.273\\n0.346\\n0.427\\n0.431\\n0.478\\n1.195\\n0.891\\nTraffic\\n0.432\\n0.303\\n0.440\\n0.310\\n0.447\\n0.313\\n0.430\\n0.305\\n0.663\\n0.425\\n0.749\\n0.446\\n1.534\\n0.811\\nTable 4: Few-shot long-term forecasting using 5% of the training data. Results are averaged across four prediction lengths:\\nT ∈ {96, 192, 336, 720} for all datasets. The best average results are in bold, while the second-best are underlined.\\nMethods\\nLLM4TS\\nGPT4TS\\nDLinear\\nPatchTST\\nFEDformer\\nAutoformer\\nInformer\\nMetric\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nWeather\\n0.256\\n0.292\\n0.264\\n0.302\\n0.264\\n0.309\\n0.270\\n0.304\\n0.310\\n0.353\\n0.311\\n0.354\\n0.584\\n0.528\\nETTh1\\n0.651\\n0.551\\n0.682\\n0.560\\n0.750\\n0.611\\n0.695\\n0.569\\n0.659\\n0.562\\n0.722\\n0.599\\n1.225\\n0.817\\nETTh2\\n0.359\\n0.405\\n0.401\\n0.434\\n0.828\\n0.616\\n0.439\\n0.448\\n0.441\\n0.457\\n0.470\\n0.489\\n3.923\\n1.654\\nETTm1\\n0.413\\n0.417\\n0.472\\n0.450\\n0.401\\n0.417\\n0.527\\n0.476\\n0.731\\n0.593\\n0.796\\n0.621\\n1.163\\n0.792\\nETTm2\\n0.286\\n0.332\\n0.308\\n0.346\\n0.399\\n0.426\\n0.315\\n0.353\\n0.381\\n0.405\\n0.389\\n0.434\\n3.659\\n1.490\\nECL\\n0.173\\n0.266\\n0.179\\n0.273\\n0.177\\n0.276\\n0.181\\n0.277\\n0.267\\n0.353\\n0.346\\n0.405\\n1.281\\n0.930\\nTraffic\\n0.418\\n0.295\\n0.434\\n0.305\\n0.451\\n0.317\\n0.418\\n0.297\\n0.677\\n0.424\\n0.833\\n0.502\\n1.591\\n0.832\\nTable 5: Self-supervised learning evaluation in forecasting with linear probing. We use prediction lengths T\\n∈\\n{24, 48, 168, 336, 720} for the ETTh1 dataset. The best average results are in bold, while the second-best are underlined.\\nMethods\\nLLM4TS\\nPatchTST\\nBTSF\\nTS2Vec\\nTNC\\nTS-TCC\\nMetric\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nETTh1\\n24\\n0.315\\n0.365\\n0.322\\n0.369\\n0.541\\n0.519\\n0.599\\n0.534\\n0.632\\n0.596\\n0.653\\n0.61\\n48\\n0.342\\n0.384\\n0.354\\n0.385\\n0.613\\n0.524\\n0.629\\n0.555\\n0.705\\n0.688\\n0.72\\n0.693\\n168\\n0.401\\n0.415\\n0.419\\n0.424\\n0.64\\n0.532\\n0.755\\n0.636\\n1.097\\n0.993\\n1.129\\n1.044\\n336\\n0.421\\n0.427\\n0.445\\n0.446\\n0.864\\n0.689\\n0.907\\n0.717\\n1.454\\n0.919\\n1.492\\n1.076\\n720\\n0.426\\n0.447\\n0.487\\n0.478\\n0.993\\n0.712\\n1.048\\n0.79\\n1.604\\n1.118\\n1.603\\n1.206\\nAvg.\\n0.381\\n0.408\\n0.405\\n0.420\\n0.730\\n0.595\\n0.788\\n0.646\\n1.098\\n0.863\\n1.119\\n0.926\\n5.2\\nFew-Shot Learning\\nPretrained LLMs excel in few-shot learning due to their in-\\nherent knowledge (Brown et al. 2020), and we strive to repli-\\ncate this success in long-term time-series forecasting. In our\\nexperiments, we maintain consistent splits for training, vali-\\ndation, and test sets in both standard learning (where the full\\ntraining set is used) and few-shot learning. For few-shot sce-\\nnarios, we limit the training data percentage. To showcase\\nthe effectiveness of few-shot learning with LLM4TS, we\\npresent results using 10% of the data in Table 3 and results\\nusing 5% of the data in Table 4. Both LLM4TS and GPT4TS\\n(Zhou et al. 2023) consistently outperform most competitors\\nacross various datasets, thanks to the knowledge embedded\\nin GPT-2. Notably, LLM4TS outperforms GPT4TS on every\\ndataset without exception.\\n5.3\\nSelf-Supervised Learning\\nGiven that autoregressive is a prevalent self-supervised\\nlearning technique, we aim to assess the representation\\nlearning capabilities of the sft model. To evaluate the effec-\\ntiveness of self-supervised learning, we use linear probing\\nafter completing this stage. With the backbone model’s pa-\\nrameters fixed, obtaining strong performance on the down-\\nstream task requires highly expressive learned representa-\\ntions. We use the ETTh1 dataset for long-term time-series\\nforecasting to evaluate the effectiveness of learned represen-\\ntation. As showcased in Table 5, LLM4TS outperforms the\\ncompetition in this assessment.\\n5.4\\nAblation Study\\nSupervised Fine-Tuning, Temporal Encoding, and PEFT\\nIn Table 6, we examine the impact of three key compo-\\nnents: supervised fine-tuning, temporal encoding, and PEFT\\non forecasting. Both standard and few-shot learning ap-\\nproaches are evaluated on the ETTh1 dataset. A comparative\\nanalysis of results—with and without these three compo-\\nnents—highlights their individual importance in enhancing\\nforecasting accuracy in both full-data and limited-data sce-\\nnarios. Notably, LLM4TS delivers exceptional performance\\nTable 6: Ablation study of supervised fine-tuning, temporal encoding, and PEFT in LLM4TS. Each ablation is conducted\\nunder both standard learning and few-shot learning with 10% training data. IMP. denotes the average improvement achieved by\\nincorporating one of LLM4TS’s core components. We use prediction lengths T ∈ {96, 192, 336, 720} for the ETTh1 dataset.\\nThe best average results are in bold.\\nMethods\\nIMP.\\nLLM4TS\\nw/o Supervised Fine-Tuning\\nw/o Temporal Encoding\\nw/o PEFT\\nMetric\\nMSE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nETTh1 (full)\\n96\\n0.89%\\n0.371\\n0.394\\n0.372\\n0.395\\n0.378\\n0.397\\n0.373\\n0.393\\n192\\n1.14%\\n0.403\\n0.412\\n0.404\\n0.411\\n0.411\\n0.416\\n0.408\\n0.412\\n336\\n1.16%\\n0.42\\n0.422\\n0.422\\n0.423\\n0.433\\n0.43\\n0.42\\n0.421\\n720\\n2.21%\\n0.422\\n0.444\\n0.424\\n0.448\\n0.442\\n0.457\\n0.429\\n0.45\\nAvg.\\n1.37%\\n0.404\\n0.418\\n0.406\\n0.419\\n0.416\\n0.425\\n0.408\\n0.419\\nETTh1 (10%)\\n96\\n1.80%\\n0.417\\n0.432\\n0.43\\n0.438\\n0.422\\n0.434\\n0.422\\n0.433\\n192\\n1.01%\\n0.469\\n0.468\\n0.488\\n0.474\\n0.463\\n0.465\\n0.471\\n0.462\\n336\\n4.03%\\n0.505\\n0.499\\n0.538\\n0.506\\n0.516\\n0.508\\n0.525\\n0.504\\n720\\n11.89%\\n0.708\\n0.572\\n0.762\\n0.589\\n0.714\\n0.584\\n0.98\\n0.672\\nAvg.\\n6.20%\\n0.525\\n0.493\\n0.555\\n0.502\\n0.529\\n0.498\\n0.600\\n0.518\\nTable 7: Ablation study of training strategies during downstream fine-tuning. Each ablation is conducted under both standard\\nlearning and few-shot learning with 10% training data. IMP. denotes the average improvement achieved by incorporating one\\nof LLM4TS’s core components. We use prediction lengths T ∈ {96, 192, 336, 720} for the ETTh1 dataset. The best average\\nresults are in bold.\\nMethods\\nIMP.\\nLLMTS w/ LP FT\\nLLMTS w/ FT\\nLLMTS w/ LP\\nMetric\\nMSE\\nMSE\\nMAE\\nMSE\\nMAE\\nMSE\\nMAE\\nETTh1 (full)\\n96\\n0.80%\\n0.371\\n0.394\\n0.371\\n0.394\\n0.377\\n0.398\\n192\\n0.98%\\n0.403\\n0.412\\n0.404\\n0.413\\n0.41\\n0.416\\n336\\n0.70%\\n0.42\\n0.422\\n0.42\\n0.423\\n0.426\\n0.424\\n720\\n0.35%\\n0.422\\n0.444\\n0.42\\n0.444\\n0.427\\n0.447\\nAvg.\\n0.70%\\n0.404\\n0.418\\n0.404\\n0.419\\n0.410\\n0.421\\nETTh1 (10%)\\n96\\n0.12%\\n0.421\\n0.435\\n0.423\\n0.436\\n0.42\\n0.433\\n192\\n2.52%\\n0.454\\n0.457\\n0.477\\n0.474\\n0.455\\n0.454\\n336\\n2.36%\\n0.515\\n0.504\\n0.545\\n0.524\\n0.511\\n0.507\\n720\\n3.92%\\n0.711\\n0.574\\n0.743\\n0.589\\n0.737\\n0.589\\nAvg.\\n2.51%\\n0.525\\n0.493\\n0.547\\n0.506\\n0.531\\n0.496\\nin few-shot learning, averaging a 6.2% reduction in MSE\\nwith each incorporation of these components.\\nTraining Strategies in Downstream Fine-Tuning\\nWhen\\npre-trained models are robust and possess embedded knowl-\\nedge, fully fine-tuning (FT) them on a downstream task\\nmight distort the learned representation, especially when a\\ndistribution shift exists between the training and test sets.\\nA straightforward solution is to first apply linear probing\\n(LP), followed by full fine-tuning. Empirically, the com-\\nbined approach (LP-FT) consistently surpasses either LP or\\nFT alone, regardless of the magnitude of the distribution\\nshift (Kumar et al. 2022). For our experiments, we employ\\nlinear probing for the initial half of the training epochs and\\ntransition to full fine-tuning for the latter half. This method\\nenhances the performance of the dft models in both standard\\nand few-shot learning, as illustrated in Table 7. We observe\\nthat few-shot learning appears to derive greater benefit from\\nthe LP-FT method compared to standard learning, likely be-\\ncause few-shot scenarios are more susceptible to significant\\ndistribution shifts. Although the benefits of using LP-FT in\\nboth scenarios might seem subtle, there are no drawbacks\\nto its adoption. The subtle improvements observed with LP-\\nFT can be linked to the fact that only a small portion of the\\nparameters in the LLM4TS’s backbone model are trainable,\\nwhich narrows the distinction between LP and FT.\\n6\\nConclusion\\nIn this paper, we introduce LLM4TS, a framework for time-\\nseries forecasting with pre-trained LLMs. We employ patch-\\ning and channel-independence techniques while integrating\\ntemporal information. Our two-stage fine-tuning methodol-\\nogy first aligns LLMs with time-series data characteristics\\nand then focuses on time-series forecasting tasks. To en-\\nhance the pre-trained LLMs’ adaptability to this new modal-\\nity of time-series data without distorting inherent features,\\nwe incorporate Layer Normalization Tuning and LoRA to\\nenhance the model’s robustness and versatility. These PEFT\\nstrategies further optimize computational efficiency dur-\\ning fine-tuning. In our evaluations, LLM4TS not only sets\\nnew benchmarks in long-term forecasting and representation\\nlearning but also excels in few-shot learning, making it a top\\nchoice for real-world scenarios with limited data availability.\\nReferences\\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.;\\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosse-\\nlut, A.; Brunskill, E.; Brynjolfsson, E.; Buch, S.; Card, D.;\\nCastellon, R.; Chatterji, N. S.; Chen, A. S.; Creel, K. A.;\\nDavis, J.; Demszky, D.; Donahue, C.; Doumbouya, M.; Dur-\\nmus, E.; Ermon, S.; Etchemendy, J.; Ethayarajh, K.; Fei-Fei,\\nL.; Finn, C.; Gale, T.; Gillespie, L. E.; Goel, K.; Goodman,\\nN. D.; Grossman, S.; Guha, N.; Hashimoto, T.; Henderson,\\nP.; Hewitt, J.; Ho, D. E.; Hong, J.; Hsu, K.; Huang, J.; Icard,\\nT. F.; Jain, S.; Jurafsky, D.; Kalluri, P.; Karamcheti, S.; Keel-\\ning, G.; Khani, F.; Khattab, O.; Koh, P. W.; Krass, M. S.; Kr-\\nishna, R.; Kuditipudi, R.; Kumar, A.; Ladhak, F.; Lee, M.;\\nLee, T.; Leskovec, J.; Levent, I.; Li, X. L.; Li, X.; Ma, T.;\\nMalik, A.; Manning, C. D.; Mirchandani, S.; Mitchell, E.;\\nMunyikwa, Z.; Nair, S.; Narayan, A.; Narayanan, D.; New-\\nman, B.; Nie, A.; Niebles, J. C.; Nilforoshan, H.; Nyarko,\\nJ. F.; Ogut, G.; Orr, L. J.; Papadimitriou, I.; Park, J. S.; Piech,\\nC.; Portelance, E.; Potts, C.; Raghunathan, A.; Reich, R.;\\nRen, H.; Rong, F.; Roohani, Y. H.; Ruiz, C.; Ryan, J.; R’e,\\nC.; Sadigh, D.; Sagawa, S.; Santhanam, K.; Shih, A.; Srini-\\nvasan, K. P.; Tamkin, A.; Taori, R.; Thomas, A. W.; Tram`er,\\nF.; Wang, R. E.; Wang, W.; Wu, B.; Wu, J.; Wu, Y.; Xie,\\nS. M.; Yasunaga, M.; You, J.; Zaharia, M. A.; Zhang, M.;\\nZhang, T.; Zhang, X.; Zhang, Y.; Zheng, L.; Zhou, K.; and\\nLiang, P. 2021. On the Opportunities and Risks of Founda-\\ntion Models. ArXiv, abs/2108.07258.\\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\\nA.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan,\\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\\nare Few-Shot Learners. In NeurIPS.\\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\\nAn Image is Worth 16x16 Words: Transformers for Image\\nRecognition at Scale. In ICLR. OpenReview.net.\\nEldele, E.; Ragab, M.; Chen, Z.; Wu, M.; Kwoh, C.; Li, X.;\\nand Guan, C. 2021. Time-Series Representation Learning\\nvia Temporal and Contextual Contrasting. In International\\nJoint Conference on Artificial Intelligence.\\nGao, P.; Han, J.; Zhang, R.; Lin, Z.; Geng, S.; Zhou, A.;\\nZhang, W.; Lu, P.; He, C.; Yue, X.; Li, H.; and Qiao, Y. J.\\n2023. LLaMA-Adapter V2: Parameter-Efficient Visual In-\\nstruction Model. ArXiv, abs/2304.15010.\\nGhosal, D.; Majumder, N.; Mehrish, A.; and Poria, S.\\n2023.\\nText-to-Audio Generation using Instruction-Tuned\\nLLM and Latent Diffusion Model. ArXiv, abs/2304.13731.\\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\\nof large language models. arXiv preprint arXiv:2106.09685.\\nKim, T.; Kim, J.; Tae, Y.; Park, C.; Choi, J.-H.; and Choo, J.\\n2021. Reversible instance normalization for accurate time-\\nseries forecasting against distribution shift. In International\\nConference on Learning Representations.\\nKumar, A.; Raghunathan, A.; Jones, R.; Ma, T.; and\\nLiang, P. 2022.\\nFine-tuning can distort pretrained fea-\\ntures and underperform out-of-distribution. arXiv preprint\\narXiv:2202.10054.\\nLialin, V.; Deshpande, V.; and Rumshisky, A. 2023. Scal-\\ning Down to Scale Up: A Guide to Parameter-Efficient Fine-\\nTuning. ArXiv, abs/2303.15647.\\nLu, K.; Grover, A.; Abbeel, P.; and Mordatch, I. 2021.\\nPretrained transformers as universal computation engines.\\narXiv preprint arXiv:2103.05247, 1.\\nNie, Y.; Nguyen, N. H.; Sinthong, P.; and Kalagnanam, J.\\n2023. A Time Series is Worth 64 Words: Long-term Fore-\\ncasting with Transformers. In ICLR. OpenReview.net.\\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\\net al. 2022. Training language models to follow instructions\\nwith human feedback. Advances in Neural Information Pro-\\ncessing Systems, 35: 27730–27744.\\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\\nSutskever, I.; et al. 2019. Language models are unsupervised\\nmultitask learners. OpenAI blog, 1: 9.\\nTonekaboni, S.; Eytan, D.; and Goldenberg, A. 2021. Unsu-\\npervised Representation Learning for Time Series with Tem-\\nporal Neighborhood Coding. In ICLR. OpenReview.net.\\nWen, Q.; Zhou, T.; Zhang, C.; Chen, W.; Ma, Z.; Yan, J.; and\\nSun, L. 2022. Transformers in time series: A survey. arXiv\\npreprint arXiv:2202.07125.\\nWu, H.; Xu, J.; Wang, J.; and Long, M. 2021. Autoformer:\\nDecomposition transformers with auto-correlation for long-\\nterm series forecasting.\\nAdvances in Neural Information\\nProcessing Systems, 34: 22419–22430.\\nYang, L.; and Hong, S. 2022. Unsupervised time-series rep-\\nresentation learning with iterative bilinear temporal-spectral\\nfusion. In International Conference on Machine Learning,\\n25038–25054. PMLR.\\nYue, Z.; Wang, Y.; Duan, J.; Yang, T.; Huang, C.; Tong, Y.;\\nand Xu, B. 2022. Ts2vec: Towards universal representation\\nof time series. In Proceedings of the AAAI Conference on\\nArtificial Intelligence, volume 36, 8980–8987.\\nZeng, A.; Chen, M.; Zhang, L.; and Xu, Q. 2023. Are trans-\\nformers effective for time series forecasting? In Proceedings\\nof the AAAI conference on artificial intelligence, volume 37,\\n11121–11128.\\nZhang, Y.; Gong, K.; Zhang, K.; Li, H.; Qiao, Y.; Ouyang,\\nW.; and Yue, X. 2023.\\nMeta-Transformer: A Unified\\nFramework for Multimodal Learning.\\narXiv preprint\\narXiv:2307.10802.\\nZhou, H.; Zhang, S.; Peng, J.; Zhang, S.; Li, J.; Xiong, H.;\\nand Zhang, W. 2021. Informer: Beyond efficient transformer\\nfor long sequence time-series forecasting. In Proceedings of\\nthe AAAI conference on artificial intelligence, volume 35,\\n11106–11115.\\nZhou, T.; Ma, Z.; Wen, Q.; Wang, X.; Sun, L.; and Jin, R.\\n2022. Fedformer: Frequency enhanced decomposed trans-\\nformer for long-term series forecasting.\\nIn International\\nConference on Machine Learning, 27268–27286. PMLR.\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
